<!DOCTYPE html>

<html>

<link rel="stylesheet" href="styles.css">

<head>
  <title>Language models are unsupervised multitask learners - note</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400;1,700&family=Source+Serif+Pro:ital,wght@0,400;0,700;1,400;1,700&display=swap"
    rel="stylesheet">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
</head>

<div id="nav">
  <a href="/">Return to home page.</a>
</div>

<div id="contact">
  Contact: Please email me at <a href="mailto:jsfi2010@hotmail.com">jsfi2010@hotmail.com</a>.
</div>

<div id="header">
  <h1>Language models are unsupervised multitask learners - note</h1>
</div>

<div id="content">
  <p>2024-11-15</p>
  <p>
    As <a href="https://docs.google.com/document/d/15e8vBVwrJP5m1Y8VytQY3wSa1Qg18OM43YqP2-Wws1c/edit?tab=t.0">suggested by Charlie Rogers-Smith</a>, I've started reading papers and trying to crystallize my understanding by writing summaries and attempting to replicate the technical work.
  </p>

  <p>
    As I am having some trouble understanding <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">this article</a>, I'm hopeful that writing a summary of my thoughts can help me consolidate what I'm thinking about. It may be that what is holding me back is that I'm not too familiar with a lot of what the article describes (attention mechanism, Transformer architecture,
  </p>

  <p>
    The authors describe the nature of language models. Language models attempt to learn the occurrence of a word given a previous sequence of words. The author describes the approach of a traditional paradigm in machine learning in which a system is trained on certain examples that correspond to a certain task. In contrast, the author mentions research that has been done on multitask learning. In this paradigm, a system is trained on multiple (dataset, task) sets. It might be important to make sure that language models are generalizable.
  </p>

  <p>
    Author describes zero-shot learning. It refers to systems that need to predict the class that an example of an as of yet unseen class belongs to. A system might be able to predict that a picture of a certain animal species is a picture of a zebra given a text-based description of a zebra, even though it hasn't been trained on an image of a zebra in the past.
  </p>

  <p>
    Author concludes that language models that are trained on "large and diverse" datasets seem to be able to perform well on a great diversity of models.
  </p>
</div>

</html>
